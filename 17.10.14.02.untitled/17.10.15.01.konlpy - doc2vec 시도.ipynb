{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\", \"malgun gothic\";\n",
       "        src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "    }\n",
       "//    div.cell{\n",
       "//        width: 100%;\n",
       "//    }\n",
       "    // 아래의 div.container는 내가 임의로 추가한 style임\n",
       "//    div.container{\n",
       "//        width: 105%;\n",
       "//    }\n",
       "    ul {\n",
       "        line-height: 100%;\n",
       "        font-size: 100%;\n",
       "    }\n",
       "    li {\n",
       "        margin-bottom: 0.5em;\n",
       "        line-height: 1.2em;\n",
       "    }\n",
       "    ul.li {\n",
       "        margin-bottom: 0.5em;\n",
       "        line-height: 1.2em;\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: Helvetica, serif, \"malgun gothic\";\n",
       "    }\n",
       "    h4{\n",
       "        margin-top: 12px;\n",
       "//        margin-bottom: 3px;\n",
       "       }\n",
       "    div.text_cell_render{\n",
       "//        font-family: Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "        font-family: malgun gothic;\n",
       "//        line-height: 140%;\n",
       "        line-height: 1.5em;\n",
       "//        font-size: 100%;\n",
       "//        width: 100%;\n",
       "//        margin-left:auto;\n",
       "//        margin-right:auto;\n",
       "    }\n",
       "    .CodeMirror{\n",
       "            font-family: \"Source Code Pro\", source-code-pro,Consolas, monospace;\n",
       "    }\n",
       "/*    .prompt{\n",
       "        display: None;\n",
       "    }*/\n",
       "    .text_cell_render h5 {\n",
       "        font-family: malgun gothic;\n",
       "        font-weight: 300;\n",
       "        font-size: 16pt;\n",
       "        color: #4057A1;\n",
       "        font-style: italic;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "        }\n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "//                \"HTML-CSS\": {\n",
       "//                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "//                }\n",
       "        });\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mac475의 ipython 표준 style을 적용함\n",
    "from IPython.core.display import HTML\n",
    "styles = open(\"../resources/styles/custom.css\", \"r\", encoding='utf-8').read()\n",
    "HTML( styles )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  Pretty Display of Variables를 적용하여 중간 결과를 확인하고자 함\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# matplotlib plot내 한글의 표현위한 설정\n",
    "from matplotlib import font_manager, rc\n",
    "font_fname = 'c:/windows/fonts/malgun.ttf'     # A font of your choice\n",
    "font_name = font_manager.FontProperties(fname=font_fname).get_name()\n",
    "rc('font', family=font_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 영화리뷰 data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data reading function\n",
    "def read_data(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        data = [line.split('\\t')[1:] for line in f.read().splitlines()]\n",
    "        data = data[1:]   # header 제외\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = read_data('./datasets/ratings_train.txt')\n",
    "test_data = read_data('./datasets/ratings_test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pos tagger 정의, parts of speech : 품사\n",
    "from konlpy.tag import Twitter\n",
    "pos_tagger = Twitter()\n",
    "\n",
    "def tokenize(doc):\n",
    "    # norm, stem은 optional\n",
    "    return ['/'.join(t) for t in pos_tagger.pos(doc, norm=True, stem=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lambda function의 활용 참고\n",
    "\n",
    "# list(filter(lambda x: x % 2, [0,1,2,3]))\n",
    "# list(filter(lambda x: x == 'a', ['a','b','c','d']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noun 추출만을 위한 pos tagger\n",
    "def tokenize_only_noun(doc):\n",
    "#     print(doc)\n",
    "    return ['/'.join(t) for t in list(filter(lambda x: x[1]=='Noun', pos_tagger.pos(doc, norm=True, stem=True)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# train docs 생성\n",
    "train_docs = [(tokenize(row[0]), row[1]) for row in tqdm(train_data)]\n",
    "train_docs_only_nouns = [(tokenize_only_noun(row[0]), row[1]) for row in tqdm(train_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    }
   ],
   "source": [
    "# test docs 생성\n",
    "test_docs = [(tokenize(row[0]), row[1]) for row in tqdm(test_data)]\n",
    "test_docs_only_nouns = [(tokenize_only_noun(row[0]), row[1]) for row in tqdm(test_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['아/Exclamation',\n",
       "  '더빙/Noun',\n",
       "  '../Punctuation',\n",
       "  '진짜/Noun',\n",
       "  '짜증/Noun',\n",
       "  '나다/Verb',\n",
       "  '목소리/Noun'],\n",
       " '0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(['더빙/Noun', '진짜/Noun', '짜증/Noun', '목소리/Noun'], '0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 확인\n",
    "train_docs[0]\n",
    "# test_docs[0]\n",
    "\n",
    "train_docs_only_nouns[0]\n",
    "# test_docs_only_nouns[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2194536"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2194536"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모든 document내의 pos tagging된 word를 하나의 list에 tokens로 담는다\n",
    "tokens = [t for d in train_docs for t in d[0]]\n",
    "len(tokens)\n",
    "\n",
    "# nouns만 한정하여, 모든 document내의 pos tagging된 word를 하나의 list에 tokens로 담는다\n",
    "tokens_only_nouns = [t for d in train_docs_only_nouns for t in d[0]]\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. doc2vec 기반 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### doc2vec concept\n",
    "\n",
    "- 참고 : https://stackoverflow.com/questions/42827175/gensim-what-is-difference-between-word2vec-and-doc2vec\n",
    "\n",
    "    >In word2vec, you train to find word vectors and then run similarity queries between words.\n",
    "    \n",
    "    >In doc2vec, you tag your text and you also get tag vectors. For instance, you have different documents from different authors and use authors as tags on documents. Then, after doc2vec training you can use the same vector aritmetics to run similarity queries on author tags: i.e who are the most similar authors to AUTHOR_X? If two authors generally use the same words then their vector will be closer. AUTHOR_X is not a real word which is part of your corpus just something you determine. So you don't need to have it or manually insert it into your text. Gensim allows you to train doc2vec with or without word vectors (i.e. if you only care about tag similarities between each other).\n",
    "\n",
    "    >Here is a good presentation (https://www.youtube.com/watch?v=vkfXBGnDplQ) on word2vec basics and how they use doc2vec in an innovative way for product recommendations (related blog post).\n",
    "    \n",
    "참고 : https://stackoverflow.com/questions/42781292/doc2vec-get-most-similar-documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['아/Exclamation',\n",
       "  '더빙/Noun',\n",
       "  '../Punctuation',\n",
       "  '진짜/Noun',\n",
       "  '짜증/Noun',\n",
       "  '나다/Verb',\n",
       "  '목소리/Noun'],\n",
       " '0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(['더빙/Noun', '진짜/Noun', '짜증/Noun', '목소리/Noun'], '0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_docs[0]\n",
    "train_docs_only_nouns[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "TaggedDocument = namedtuple('TaggedDocument', 'words tags')\n",
    "\n",
    "# 여기서는 15만개 training documents 전부 사용함\n",
    "cnt = 150000\n",
    "tagged_train_docs = [TaggedDocument(d, [c]) for d, c in tqdm(train_docs[:cnt])]\n",
    "tagged_test_docs = [TaggedDocument(d, [c]) for d, c in tqdm(test_docs[:cnt])]\n",
    "\n",
    "tagged_train_docs_only_nouns = [TaggedDocument(d, [c]) for d, c in tqdm(train_docs_only_nouns[:cnt])]\n",
    "tagged_test_docs_only_nouns = [TaggedDocument(d, [c]) for d, c in tqdm(test_docs_only_nouns[:cnt])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import doc2vec\n",
    "\n",
    "# 사전 구축 : 모든 pos\n",
    "doc_vectorizer = doc2vec.Doc2Vec(size=300, alpha=0.025, min_alpha=0.025, seed=1234)\n",
    "doc_vectorizer.build_vocab(tagged_train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tagged_train_docs[13]\n",
    "# tagged_train_docs_only_nouns[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 구축 : noun만 포함하는 pos\n",
    "doc_vectorizer_only_nouns = doc2vec.Doc2Vec(size=500, alpha=0.025, min_alpha=0.025, seed=1235)\n",
    "doc_vectorizer_only_nouns.build_vocab(tagged_train_docs_only_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['담/Noun', '신문/Noun', '기사/Noun', '로만/Noun', '자꾸/Noun', '그/Noun', '사람/Noun', '것/Noun'], tags=['1'])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['담/Noun', '신문/Noun', '기사/Noun', '로만/Noun', '자꾸/Noun', '그/Noun', '사람/Noun', '것/Noun'], tags=['1'])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_train_docs_only_nouns[13]\n",
    "tagged_train_docs_only_nouns[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150000"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tagged_train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                   | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1872079"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████▎                                      | 1/10 [00:04<00:44,  4.90s/it]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1872739"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████▌                                  | 2/10 [00:09<00:39,  4.90s/it]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3743266"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|████████████▉                              | 3/10 [00:19<00:44,  6.34s/it]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5616065"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████▏                         | 4/10 [00:34<00:53,  8.95s/it]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7488084"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████▌                     | 5/10 [00:54<01:00, 12.15s/it]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9362323"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|█████████████████████████▊                 | 6/10 [01:18<01:03, 15.78s/it]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11233019"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|██████████████████████████████             | 7/10 [01:47<00:59, 19.71s/it]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13103870"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|██████████████████████████████████▍        | 8/10 [02:21<00:47, 23.92s/it]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14979658"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|██████████████████████████████████████▋    | 9/10 [02:59<00:28, 28.27s/it]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16852400"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    }
   ],
   "source": [
    "# Train document vectors!\n",
    "for epoch in tqdm(range(10)):\n",
    "    doc_vectorizer.train(tagged_train_docs, epochs=epoch, total_examples=len(tagged_train_docs))\n",
    "    doc_vectorizer.alpha -= 0.002  # decrease the learning rate\n",
    "    doc_vectorizer.min_alpha = doc_vectorizer.alpha  # fix the learning rate, no decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                   | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "950500"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████▎                                      | 1/10 [00:04<00:36,  4.07s/it]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "950480"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████▌                                  | 2/10 [00:08<00:32,  4.08s/it]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1900972"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|████████████▉                              | 3/10 [00:16<00:37,  5.30s/it]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2850808"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████▏                         | 4/10 [00:28<00:44,  7.38s/it]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3802124"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████▌                     | 5/10 [00:45<00:50, 10.15s/it]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4752568"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|█████████████████████████▊                 | 6/10 [01:05<00:53, 13.32s/it]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5701808"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|██████████████████████████████             | 7/10 [01:30<00:50, 16.72s/it]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6653345"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|██████████████████████████████████▍        | 8/10 [01:59<00:40, 20.30s/it]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7603383"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|██████████████████████████████████████▋    | 9/10 [02:32<00:24, 24.13s/it]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8553417"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    }
   ],
   "source": [
    "# Train document vectors!\n",
    "for epoch in tqdm(range(10)):\n",
    "    doc_vectorizer_only_nouns.train(tagged_train_docs_only_nouns, epochs=epoch, total_examples=len(tagged_train_docs_only_nouns))\n",
    "    doc_vectorizer_only_nouns.alpha -= 0.002  # decrease the learning rate\n",
    "    doc_vectorizer_only_nouns.min_alpha = doc_vectorizer_only_nouns.alpha  # fix the learning rate, no decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To save\n",
    "doc_vectorizer.save('./model/17.10.15.02.doc2vec.model')\n",
    "\n",
    "# To save\n",
    "doc_vectorizer_only_nouns.save('./model/17.10.15.02.doc2vec.only.nouns.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('로드무비/Noun', 0.39678576588630676),\n",
       " ('로맨틱/Noun', 0.37818723917007446),\n",
       " ('가족영화/Noun', 0.3733956217765808),\n",
       " ('정통/Noun', 0.35419774055480957),\n",
       " ('섹슈얼/Noun', 0.34228917956352234),\n",
       " ('러브스토리/Noun', 0.3355391323566437),\n",
       " ('액션영화/Noun', 0.3314771354198456),\n",
       " ('영화장르/Noun', 0.33072495460510254),\n",
       " ('코메디/Noun', 0.330555260181427),\n",
       " ('멜로/Noun', 0.3128451704978943)]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_vectorizer.most_similar(positive=['영화/Noun','로맨스/Noun','장르/Noun'], negative=['공포/Noun'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('정통/Noun', 0.3654744625091553),\n",
       " ('로맨틱/Noun', 0.354615718126297),\n",
       " ('로드무비/Noun', 0.32557517290115356),\n",
       " ('멜로/Noun', 0.3230037987232208),\n",
       " ('코메디/Noun', 0.3103031516075134),\n",
       " ('주변인/Noun', 0.31011444330215454),\n",
       " ('남여/Noun', 0.3047192096710205),\n",
       " ('러브스토리/Noun', 0.30434930324554443),\n",
       " ('영국영화/Noun', 0.3023414611816406),\n",
       " ('로코/Noun', 0.30176958441734314)]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_vectorizer_only_nouns.most_similar(positive=['영화/Noun','로맨스/Noun','장르/Noun'], negative=['공포/Noun'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_vectorizer.docvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0', 0.3540586531162262)]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # doc_vectorizer.infer_vector(doc_vectorizer.infer_vector('영화/Noun,로맨스/Noun,장르/Noun'.split()))\n",
    "\n",
    "# # token_question = '더빙/Noun 진짜/Noun 짜증/Noun 목소리/Noun 짜증/Noun '.split()\n",
    "# token_question = '짜증/Noun'.split()\n",
    "# new_vector = doc_vectorizer_only_nouns.infer_vector(token_question)\n",
    "# # doc_vectorizer.infer_vector(tokens_test)\n",
    "\n",
    "# doc_vectorizer_only_nouns.docvecs.most_similar([new_vector], topn=1) #gives you top 10 document tags and their cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = [doc_vectorizer.infer_vector(doc.words) for doc in tagged_train_docs]\n",
    "train_y = [doc.tags[0] for doc in tagged_train_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(train_x)       # 사실 이 때문에 앞의 term existance와는 공평한 비교는 아닐 수 있다\n",
    "# => 150000\n",
    "len(train_x[0])\n",
    "# => 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_only_nouns_x = [doc_vectorizer_only_nouns.infer_vector(doc.words) for doc in tagged_train_docs_only_nouns]\n",
    "train_only_nouns_y = [doc.tags[0] for doc in tagged_train_docs_only_nouns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150000"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_only_nouns_x)       # 사실 이 때문에 앞의 term existance와는 공평한 비교는 아닐 수 있다\n",
    "# => 150000\n",
    "len(train_only_nouns_x[0])\n",
    "# => 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_x = [doc_vectorizer.infer_vector(doc.words) for doc in tagged_test_docs]\n",
    "test_y = [doc.tags[0] for doc in tagged_test_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_x)\n",
    "# => 50000\n",
    "len(test_x[0])\n",
    "# => 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_only_nouns_x = [doc_vectorizer_only_nouns.infer_vector(doc.words) for doc in tagged_test_docs]\n",
    "test_only_nouns_y = [doc.tags[0] for doc in tagged_test_docs_only_nouns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_only_nouns_x)\n",
    "# => 50000\n",
    "len(test_only_nouns_x[0])\n",
    "# => 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=1234, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.63185999999999998"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state=1234)\n",
    "\n",
    "classifier.fit(train_x, train_y)\n",
    "classifier.score(test_x, test_y)\n",
    "# => 0.78246000000000004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=1234, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.6401"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(train_only_nouns_x, train_only_nouns_y)\n",
    "classifier.score(test_only_nouns_x, test_only_nouns_y)\n",
    "# => 0.78246000000000004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ?doc_vectorizer.docvecs.most_similar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# doc_vectorizer.docvecs.most_similar([new_vector]) #gives you top 10 document tags and their cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 다음은 이것을 확인할 것\n",
    "\n",
    "# https://www.lucypark.kr/courses/2015-ba/text-mining.html#3-load-tokens-with-nltktext"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
